# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wsNLBpkDHBd3s--PFmQV-a1oxCuT5Lli
"""

query = 'who is Michael Jackson'
print('query: ' + query)

# google search method

import requests
import string
# from lxml import html
from googlesearch import search
from bs4 import BeautifulSoup


# to search
# print(chatbot_query('how old is samuel l jackson'))

def chatbot_query(query, size=1):
    fallback = 'Sorry, I cannot think of a reply for that.'
    resultlist = []
    articlelist = []

    try:
        search_result_list = list(search(query, tld="fi", num=10, stop=size, pause=1))

        for i in range(0, size):
            page = requests.get(search_result_list[i])

            # tree = html.fromstring(page.content)

            soup = BeautifulSoup(page.content, features="lxml")

            article_text = ''
            article = soup.findAll('p')
            for element in article:
                article_text += '\n' + ''.join(element.findAll(text=True))
            article_text = article_text.replace('\n', '')
            first_sentence = article_text.split('.')
            first_sentence = first_sentence[0].split('?')[0]

            chars_without_whitespace = first_sentence.translate(
                {ord(c): None for c in string.whitespace}
            )

            if len(chars_without_whitespace) > 0:
                result = first_sentence
            else:
                result = fallback
            resultlist.append(result)
            articlelist.append(article_text)
        return articlelist  # instead of resultlist
    except:
        # if len(result) == 0:
        result = [fallback]
        return result

# performing google search
print('performing google search...')
print('results:')

resultlist = chatbot_query(query, size=10)
for r in resultlist:
    print(r)

# transforming to pandas DataFrame
print('transforming to pandas DataFrame')

import pandas as pd

resultlist = pd.DataFrame(resultlist, columns=['a'])
print(resultlist)

import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from nltk.stem.porter import *
import numpy as np
np.random.seed(2018)

import nltk
nltk.download('wordnet')

stemmer = SnowballStemmer('english')

def lemmatize_stemming(text):
    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))

def preprocess(text):
    result = []
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:
            result.append(lemmatize_stemming(token))
    return result

# preprocess data
print('preprocess data: lemmatize and stem...')
print('result:')
processed_docs2 = resultlist['a'].map(preprocess)
processed_docs2

# create dictionary
dictionary2 = gensim.corpora.Dictionary(processed_docs2)

print('extract from the dictionary:')
count = 0
for k, v in dictionary2.iteritems():
    print(k, v)
    count += 1
    if count > 10:
        break

# filter out words that are contained in more than half of the documents
dictionary2.filter_extremes(no_above=0.5)

#
bow_corpus2 = [dictionary2.doc2bow(doc) for doc in processed_docs2]
print('preview bag of words for document 8 as an example:')
bow_corpus2[7]

print('to better explain:')
bow_doc_7 = bow_corpus2[7]

for i in range(len(bow_doc_7)):
    print("Word {} (\"{}\") appears {} time.".format(bow_doc_7[i][0], 
                                                     dictionary2[bow_doc_7[i][0]], 
                                                     bow_doc_7[i][1]))

# tfidf

from gensim import corpora, models
from pprint import pprint

tfidf2 = models.TfidfModel(bow_corpus2)
corpus_tfidf2 = tfidf2[bow_corpus2]
for doc in corpus_tfidf2:
    pprint(doc)
    break

# lda using bag of words
print('performing lda using bag of words, specification: 10 topics')
lda_model2 = gensim.models.LdaMulticore(bow_corpus2, num_topics=10, id2word=dictionary2, passes=2, workers=2)
for idx, topic in lda_model2.print_topics(-1):
    print('Topic: {} \nWords: {}'.format(idx, topic))

print('performing lda using tfidf, specification: 10 topics')
lda_model_tfidf2 = gensim.models.LdaMulticore(corpus_tfidf2, num_topics=10, id2word=dictionary2, passes=2, workers=4)
for idx, topic in lda_model_tfidf2.print_topics(-1):
    print('Topic: {} Word: {}'.format(idx, topic))

print('quick look at bag of words of document 6 as an example:')
processed_docs2[5]

for index, score in sorted(lda_model2[bow_corpus2[4]], key=lambda tup: -1*tup[1]):
    print("\nScore: {}\t \nTopic: {}".format(score, lda_model2.print_topic(index, 10)))

for index, score in sorted(lda_model_tfidf2[bow_corpus2[4]], key=lambda tup: -1*tup[1]):
    print("\nScore: {}\t \nTopic: {}".format(score, lda_model_tfidf2.print_topic(index, 10)))